# å¤§æ¨¡å‹æ¨ç†

æ¨ç†æ˜¯æŒ‡ç”¨ç»è¿‡è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æ¥è¿›è¡Œé¢„æµ‹æˆ–ç”Ÿæˆå›ç­”çš„è¿‡ç¨‹ã€‚å°½ç®¡æ¨ç†çœ‹ä¼¼ç®€å•ç›´æ¥ï¼Œä½†è¦å¤§è§„æ¨¡é«˜æ•ˆéƒ¨ç½²æ¨¡å‹ï¼Œå°±éœ€è¦ä»”ç»†è€ƒé‡è¯¸å¦‚æ€§èƒ½ã€æˆæœ¬å’Œå¯é æ€§ç­‰å¤šç§å› ç´ ã€‚ç”±äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è§„æ¨¡å’Œè®¡ç®—éœ€æ±‚ï¼Œæ¨¡å‹æ¨ç†å­˜åœ¨ç€ä¸€äº›ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚

è¿™é‡Œï¼Œæˆ‘ä»¬å€ŸåŠ© [`transformers`](https://huggingface.co/docs/transformers/index) å’Œ [`text-generation-inference`](https://github.com/huggingface/text-generation-inference) æ¥æ¢ç´¢ç®€å•çš„æ¨ç†æ–¹æ³•ä»¥åŠé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒçš„æ¨ç†æ–¹æ³•ã€‚é’ˆå¯¹ç”Ÿäº§ç¯å¢ƒçš„æ¨¡å‹éƒ¨ç½²ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ Text Generation Inference è¿™ä¸ªå·¥å…·åŒ…ï¼ˆç®€ç§° TGIï¼‰ï¼Œå®ƒæä¾›äº†ä¼˜åŒ–è¿‡çš„æœåŠ¡èƒ½åŠ›ã€‚

## ç« èŠ‚æ¦‚è§ˆ

å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šç®€å•çš„ä½¿ç”¨ pipeline è¿›è¡Œæ¨ç†ï¼Œè¿™é€‚ç”¨äºå¼€å‘å’Œæµ‹è¯•é˜¶æ®µï¼›ä¼˜åŒ–è¿‡çš„æœåŠ¡çº§æ¨ç†æ–¹æ¡ˆï¼Œè€…é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²ã€‚æˆ‘ä»¬å°†ä¼šè®²è§£è¿™ä¸¤ç±»æ–¹æ³•ï¼Œä»ç®€å•çš„ pipeline æ–¹æ³•å¼€å§‹ï¼Œå†é€æ­¥æ·±å…¥åˆ°ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ–¹æ¡ˆã€‚

## å†…å®¹ç›®å½•

### 1. [åŸºæœ¬çš„ Pipeline æ¨ç†](./inference_pipeline_cn.md)

è¿™ä¸€èŠ‚ä½ å°†äº†è§£ä½¿ç”¨ Hugging Face Transformers çš„ pipeline è¿›è¡ŒåŸºæœ¬çš„æ¨ç†æ–¹æ³•ã€‚æˆ‘ä»¬å°†è®²è§£ pipeline çš„è®¾ç½®ã€ç”Ÿæˆå‚æ•°çš„é…ç½®ï¼Œä»¥åŠæœ¬åœ°éƒ¨ç½²çš„æœ€ä½³å®è·µã€‚è¯¥æ–¹æ³•é€‚ç”¨äºåŸå‹è®¾è®¡å’Œå°è§„æ¨¡åº”ç”¨ã€‚

### 2. [ä½¿ç”¨ TGI è¿›è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²](./text_generation_inference_cn.md)

è¿™ä¸€èŠ‚ä½ è®²å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Text Generation Inference è¿™ä¸ªå·¥å…·åŒ…è¿›è¡Œç”Ÿäº§ç¯å¢ƒçš„æ¨¡å‹éƒ¨ç½²ã€‚æˆ‘ä»¬å°†æ¢ç´¢æœåŠ¡ç«¯æ¨¡å‹éƒ¨ç½²çš„ä¼˜åŒ–æŠ€æœ¯ã€ç»„ batch æ¨ç†çš„ç­–ç•¥ï¼Œä»¥åŠå¦‚ä½•ç›‘æ§ã€‚TGI æä¾›äº†å¾ˆå¤šé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒçš„åŠŸèƒ½ï¼Œå¦‚å¥åº·ç›‘æµ‹ã€è¯„ä¼°æŒ‡æ ‡ã€Docker éƒ¨ç½²ç­‰ã€‚

### ç»ƒä¹ 

| Title | Description | Exercise | Link | Colab |
|-------|-------------|----------|------|-------|
| Pipeline Inference | Basic inference with transformers pipeline | ğŸ¢ Set up a basic pipeline <br> ğŸ• Configure generation parameters <br> ğŸ¦ Create a simple web server | [Link](./notebooks/basic_pipeline_inference.ipynb) | [Colab](https://githubtocolab.com/huggingface/smol-course/tree/main/7_inference/notebooks/basic_pipeline_inference.ipynb) |
| TGI Deployment | Production deployment with TGI | ğŸ¢ Deploy a model with TGI <br> ğŸ• Configure performance optimizations <br> ğŸ¦ Set up monitoring and scaling | [Link](./notebooks/tgi_deployment.ipynb) | [Colab](https://githubtocolab.com/huggingface/smol-course/tree/main/7_inference/notebooks/tgi_deployment.ipynb) |

## Resources

- [Hugging Face Pipeline Tutorial](https://huggingface.co/docs/transformers/en/pipeline_tutorial)
- [Text Generation Inference Documentation](https://huggingface.co/docs/text-generation-inference/en/index)
- [Pipeline WebServer Guide](https://huggingface.co/docs/transformers/en/pipeline_tutorial#using-pipelines-for-a-webserver)
- [TGI GitHub Repository](https://github.com/huggingface/text-generation-inference)
- [Hugging Face Model Deployment Documentation](https://huggingface.co/docs/inference-endpoints/index)
- [vLLM: High-throughput LLM Serving](https://github.com/vllm-project/vllm)
- [Optimizing Transformer Inference](https://huggingface.co/blog/optimize-transformer-inference)
